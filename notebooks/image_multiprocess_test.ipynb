{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiprocessing for map data\n",
    "\n",
    "In this notebook we will demonstrate how to apply multiprocessing for images with map data.\n",
    "\n",
    "First we will demonstrate the transformation we want to apply, and then we will run it on multiple map sections at the same time with multiprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import multiprocessing\n",
    "import pickle\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = Path(\"../data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we want to do is load a TIFF image with the data for a map layer, process it and then save the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Example process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = 2000000000  # Increase PIL image load limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 40000)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"Hansen_GFC-2018-v1.6_treecover2000_50N_010W.tif\"\n",
    "granule = Image.open(data_folder/\"hansen\"/filename)\n",
    "granule.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "granule_resized = granule.resize((1000, 1000), PIL.Image.LINEAR)\n",
    "granule_array = np.array(granule_resized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the image to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_filename = filename[:-3]+\"pickle\"\n",
    "with open(data_folder/\"processed/hansen\"/processed_filename, \"wb\") as file:\n",
    "    pickle.dump(granule_array, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check that it was stored properly by loading it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "with open(data_folder/\"processed/hansen\"/processed_filename, \"rb\") as file:\n",
    "    array_check = pickle.load(file)\n",
    "print(np.all(array_check == granule_array))\n",
    "del array_check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As a function\n",
    "\n",
    "We can turn the previous process in to a function for easier use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(image):\n",
    "    # Just a simple example of a process, but non-trivial in terms of time\n",
    "    return np.array(image.resize((1000, 1000), PIL.Image.LINEAR))\n",
    "\n",
    "def image_pipeline(origin, destination=None):\n",
    "    origin=Path(origin)\n",
    "    if destination is None:\n",
    "        filename = origin.stem + \".pickle\"\n",
    "        destination = origin.parent.parent/\"processed\"/origin.parts[-2]/filename\n",
    "    loaded = Image.open(origin)\n",
    "    processed = process_image(loaded)\n",
    "    with destination.open(\"wb\") as file:\n",
    "        pickle.dump(processed, file)\n",
    "    return destination  # Just to return something"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can check that it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.08 s, sys: 624 ms, total: 7.7 s\n",
      "Wall time: 7.71 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "origin = data_folder/\"hansen\"/\"Hansen_GFC-2018-v1.6_treecover2000_50N_000E.tif\"\n",
    "image_pipeline(origin)\n",
    "\n",
    "destination = data_folder/\"processed/hansen/Hansen_GFC-2018-v1.6_treecover2000_50N_000E.pickle\"\n",
    "Path(destination).is_file()  # This checks that the file now exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing\n",
    "\n",
    "Now we use our huble pipeline with `multiprocessing`.\n",
    "\n",
    "First it would be useful to know how many CPU we have in this machine. This is easy to do with `multiprocessing.cpu_count()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 16 CPU available.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {multiprocessing.cpu_count()} CPU available.\", )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are various ways to start a multiprocess, but we are going to use the `Pool`. We need to gather of all the inputs we want to our pipeline in a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_list = list(origin.parent.glob(\"*.tif\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we define a `Pool` manager with a set number of maximum concurrent processes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_processes = multiprocessing.cpu_count() // 2  # Let's use at most half of our processors here\n",
    "pool = multiprocessing.Pool(max_processes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we execute it over all out inputs with the `.map` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 20.2 ms, sys: 6.99 ms, total: 27.2 ms\n",
      "Wall time: 9.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "results = pool.map(image_pipeline, input_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And done! It takes only slightly more time to process all the images here than to process a single one if we have enough processors in the pool."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
